---
sidebar_position: 1
date: 2025-02-01T11:13
---

# 机器学习各类算法概述

## 1.1 概述

机器学习是人工智能领域的一项关键技术，其核心理念在于通过数据构建能够自主学习并适应各种环境的模型。与传统的程序设计方法不同，机器学习模型无需人工明确指定解决问题的具体步骤和规则，而是依靠从数据中自动识别模式与规律来提取特征，并据此对未知数据进行预测或分类。

根据不同的训练方式，机器学习可以分为**监督学习**、**无监督学习**、**半监督学习**、**自监督学习**、**强化学习**等多种类型。

其中，监督学习是最为普遍采用的形式之一。该方法旨在利用已标注的数据集训练出一个函数，从而实现对未标注数据的有效预测或分类。常见的监督学习算法包括但不限于线性回归、逻辑回归、决策树、随机森林、支持向量机以及朴素贝叶斯等。

相比之下，无监督学习则不依赖于任何事先标注的信息，而是直接从未标注的数据中发现潜在的结构与特性，进而完成如聚类分析或降维处理等任务。常用的无监督学习技术有k-means算法、层次聚类法以及主成分分析等。

半监督学习作为一种介乎于监督学习与无监督学习之间的折衷方案，尝试结合少量已知标签信息与大量未知样本以提升模型的整体表现力与泛化能力。

自监督学习是一种特殊的无监督学习方法，它通过构造“伪标签”来训练模型，即在没有人工标注的情况下，利用数据本身的信息来生成标签。这种学习方法通常依赖于数据的内在结构，通过设计任务来让模型自我生成监督信号。

最后，强化学习是一种特别强调智能体与其所处环境之间互动关系的学习范式。在此框架下，智能体通过不断试验并与环境互动来优化自身的行动策略，目标是在特定情境下获取最大化的累积奖励。强化学习在诸如电子游戏、无人驾驶汽车以及机器人控制等多个前沿科技领域展现出了巨大的潜力与价值。

参考👉监督学习、无监督学习、半监督学习、自监督学习、强化学习对比分析了解更加详细的对比分析。

接下来将针对上述各类算法的工作机制、应用场景及其各自的优点与局限性做进一步详尽阐述。

后续的公式涉及到一些评价指标，可参考👉机器学习中的评价指标。

## 1.2 **线性回归**

线性回归是最简单且最常见的回归分析方法之一，主要用于**预测一个连续的目标变量（因变量）与一个或多个自变量（特征）之间的关系。它通过找到自变量和因变量之间的线性关系来建立预测模型。**

在机器学习中，线性回归常用于解决回归问题，即预测某个数值（如房价、销售额、温度等），并且它通常作为基准模型与其他更复杂的模型进行对比。

更多内容👉：[线性回归详解](/docs/机器学习/02_线性回归详解.md)

## 1.3 逻辑回归

**逻辑回归（Logistic Regression）是一种用于分类问题的统计学方法，尽管其名称中包含“回归”二字，但它实际上是一种分类模型。逻辑回归的目标是预测某一事件的概率，通常用于二分类、多分类问题（例如，预测某个邮件是否是垃圾邮件，患者是否患病等）。**

与线性回归不同，逻辑回归通过使用**逻辑函数（Logistic Function）**将线性回归的输出映射到一个介于0和1之间的概率值，从而将输出转换为分类标签（如0或1）。

更多内容👉：[逻辑回归详解](./03_逻辑回归详解.md)

## 1.4 决策树

决策树是一种常用于分类和回归任务的机器学习模型。**它通过将数据集分割成多个子集来实现预测**，每个内部节点表示一个特征的判断条件，每个叶节点表示一个最终的预测结果。

更多内容👉：[决策树详解](./04_决策树详解.md)

## 1.5 随机森林

随机森林（Random Forest，RF）是一种集成学习方法，它通过**组合多个决策树来提高模型的稳定性和预测准确性**。与单一的决策树不同，随机森林通过引入随机性来减小过拟合，并具有较强的泛化能力。

更多内容👉：[随机森林详解](./05_随机森林详解.md)

## 1.6 支持向量机

支持向量机（SVM）是一种广泛应用于分类和回归任务的监督学习算法。**SVM的核心思想是通过构造一个超平面来实现数据的分类，并尽量使得两类数据之间的间隔（margin）最大化，从而提高模型的泛化能力**。

更多内容👉：[支持向量机详解](./06_支持向量机详解.md)

## 1.7 朴素贝叶斯

朴素贝叶斯（Naive Bayes，NB）分类器是一种基于**贝叶斯定理**的概率分类方法。它的核心思想是：**通过计算各个特征在不同类别下的条件概率，进而判断某个样本属于哪个类别**。尽管其假设特征之间是相互独立的（即“朴素”假设），但在很多实际应用中，朴素贝叶斯分类器仍然表现出良好的效果，尤其是在文本分类等领域。

更多内容👉：[朴素贝叶斯详解](./07_朴素贝叶斯详解.md)

## 1.8 K近邻

**K近邻（K-Nearest Neighbor，KNN）**是一种基于实例的机器学习算法，它的基本思想是：**对于一个新的样本，KNN 会在训练数据集中找到距离该样本最近的****$K$****个邻居，然后根据这些邻居的类别来确定该样本的类别**。KNN 是一种懒学习（Lazy Learning）算法，因为它不通过训练过程来建立模型，而是直接利用训练数据进行分类或回归。

更多内容👉：[K近邻详解](./08_K近邻详解.md)

## 1.9 K-means

**K-means**是一种经典的无监督学习算法，主要用于数据的聚类分析。它的目标是将数据分成$K$个簇（cluster），使得每个簇内部的样本相似度较高，而不同簇之间的样本差异较大。K-means 聚类广泛应用于图像处理、客户细分、数据降维等场景。

更多内容👉：[K-means详解](./09_K-means详解.md)

## 1.10 层次聚类

**层次聚类**（Hierarchical Clustering）是一种常见的无监督学习聚类算法，**它通过建立一个层次结构来描述数据集中的数据点之间的关系**。与其他聚类算法（如 K-means）不同，层次聚类不需要预先指定聚类数目，而是通过逐步合并或分裂数据点，最终形成一个树状结构，通常通过**树状图**（Dendrogram）展示聚类过程。

层次聚类有两种主要类型：

- **凝聚型层次聚类（Agglomerative Hierarchical Clustering）**：从每个数据点开始，每次将距离最小的两个簇合并，直到所有数据点属于同一簇。
- **分裂型层次聚类（Divisive Hierarchical Clustering）**：从整个数据集开始，不断地将数据集分裂成更小的簇，直到每个簇只有一个数据点。

我们通常使用**凝聚型层次聚类**，因此以下介绍将以此为主。

更多内容👉：[层次聚类详解](./10_层次聚类详解.md)

## 1.11 神经网络

神经网络（Neural Network）是人工智能和深度学习领域中的一个重要概念，灵感来源于生物神经系统的结构和功能。**它由大量的神经元（节点）组成，这些神经元通过连接（权重）相互作用并传递信息，从而模拟人脑的工作原理**。神经网络在处理复杂的数据模式、分类、回归等任务中表现出了强大的能力，是许多现代人工智能技术的基础。

更多内容👉：[神经网络详解](./11_神经网络详解.md)

## 1.12 引用链接

> 1、[*https://blog.csdn.net/weixin_41147166/article/details/130343626*](https://blog.csdn.net/weixin_41147166/article/details/130343626)    
> 2、[*https://chatgpt.com/*](https://chatgpt.com/)