---
sidebar_position: 10
---

# 层次聚类

**层次聚类**（Hierarchical Clustering）是一种常见的无监督学习聚类算法，**它通过建立一个层次结构来描述数据集中的数据点之间的关系**。与其他聚类算法（如 K-means）不同，层次聚类不需要预先指定聚类数目，而是通过逐步合并或分裂数据点，最终形成一个树状结构，通常通过**树状图**（Dendrogram）展示聚类过程。

层次聚类有两种主要类型：

- **凝聚型层次聚类（Agglomerative Hierarchical Clustering）**：从每个数据点开始，每次将距离最小的两个簇合并，直到所有数据点属于同一簇。
- **分裂型层次聚类（Divisive Hierarchical Clustering）**：从整个数据集开始，不断地将数据集分裂成更小的簇，直到每个簇只有一个数据点。

我们通常使用**凝聚型层次聚类**，因此以下介绍将以此为主。

## 1.1 原理

### 1.1.1 凝聚型层次聚类（Agglomerative Hierarchical Clustering）

- **基本步骤**

    **初始化**：每个样本开始时都视为一个单独的簇，即初始时有$n$个簇。

    **计算距离**：计算当前所有簇之间的距离。常用的距离度量有欧氏距离、曼哈顿距离等。

    **合并簇**：每次选择距离最小的两个簇，将它们合并为一个新簇。

    **重复步骤 2 和 3**，直到所有样本合并成一个簇，或者达到预定的簇数。
- **簇间距离计算方法**

    在层次聚类中，不同簇之间的距离可以使用不同的度量方法。常见的簇间距离计算方法有：

    - **单链法（Single linkage）**：选择两个簇中最小的样本间距离来作为簇间距离。

$$
D\left(C_{1}, C_{2}\right)=\min \left\{d\left(x_{1}, x_{2}\right) \mid x_{1} \in C_{1}, x_{2} \in C_{2}\right\}
$$
    - **全链法（Complete linkage）**：选择两个簇中最远的样本间距离来作为簇间距离。

$$
D\left(C_{1}, C_{2}\right)=\max \left\{d\left(x_{1}, x_{2}\right) \mid x_{1} \in C_{1}, x_{2} \in C_{2}\right\}
$$
    - **平均法（Average linkage）**：计算两个簇中所有样本对的距离的平均值来作为簇间距离。

$$
D\left(C_{1}, C_{2}\right)=\frac{1}{\left|C_{1}\right| \cdot\left|C_{2}\right|} \sum_{x_{1} \in C_{1}, x_{2} \in C_{2}} d\left(x_{1}, x_{2}\right)
$$
    - **质心法（Centroid linkage）**： 计算簇的质心之间的距离，簇的质心是簇中所有点的平均位置。

$$
D\left(C_{1}, C_{2}\right)=d\left(\mu\left(C_{1}\right), \mu\left(C_{2}\right)\right)
$$

        其中，$\mu(C)$是簇$C$的质心。
- **树状图**

    层次聚类的结果通常通过树状图来展示，树状图展示了每个簇的合并顺序以及合并时的距离。通过树状图，可以清楚地观察到数据在不同层次上的聚类效果。例如，通过设置一个距离阈值，可以决定最终要分成几个簇。

## 1.2 应用

层次聚类广泛应用于多个领域，尤其是在以下几个方面具有显著的效果：

- **基因表达分析**：在生物信息学中，层次聚类用于分析基因表达数据，识别表达模式相似的基因。
- **文本聚类**：对大量文档进行聚类，可以根据文档的内容将其归为不同类别，常用于信息检索、推荐系统等。
- **图像分割**：将图像分为多个区域，每个区域内的像素具有相似的颜色或纹理，广泛应用于计算机视觉中。
- **市场细分**：通过分析消费者的行为或属性，将消费者划分为不同的群体，便于个性化营销。
- **社交网络分析**：通过分析社交网络中的用户行为和关系，发现潜在的社区结构。

## 1.3 优缺点

### 1.3.1 优点

- **无需预先指定簇数**：与 K-means 等算法不同，层次聚类不需要提前指定聚类的数量，适应性强。
- **产生层次结构**：通过树状图展示聚类的层次结构，能够提供更多的信息，便于分析。
- **适应任意形状的簇**：层次聚类能够处理不同形状的簇，而不像 K-means 那样要求簇呈圆形或球形。
- **能够识别大小不同的簇**：层次聚类适合发现大小不同的簇，而 K-means 对簇大小较为敏感。
- **易于解释**：树状图直观、易于理解，可以清楚地看到数据聚类的过程。

### 1.3.2 缺点

- **计算复杂度高**：层次聚类的时间复杂度通常为$O(n^3)$，对于大规模数据集较为耗时。
- **对噪声和离群点敏感**：层次聚类容易受到噪声和离群点的影响，可能导致不准确的聚类结果。
- **不适合大规模数据**：由于计算每一对数据点之间的距离，层次聚类在处理大规模数据时可能效率较低。
- **无法调整簇数**：一旦确定了树状图，就很难动态地调整簇数，只能通过设置一个阈值来决定聚类的数量。

### 1.3.3 改进

- **缩减计算复杂度**：一些优化方法可以减少计算量，如基于分治法的层次聚类。
- **合并相似度矩阵**：对于一些稀疏的相似度矩阵，采用有效的存储方法来减少计算开销。
- **自适应阈值**：通过调整树状图的阈值，可以动态调整聚类的精度，适应不同的需求。

## 1.4 Scikit-Learn实现

在`scikit-learn`中，层次聚类可以通过`AgglomerativeClustering` 类来实现。该类实现了凝聚型层次聚类，并提供了多种距离计算方法。

```Python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import AgglomerativeClustering
from sklearn.datasets import make_blobs
from scipy.cluster.hierarchy import dendrogram, linkage

# 生成一个简单的二维数据集
X, y = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=42)

# 创建层次聚类模型
agg_clust = AgglomerativeClustering(n_clusters=4, affinity='euclidean', linkage='ward')

# 训练模型
y_agg_clust = agg_clust.fit_predict(X)

# 可视化聚类结果
plt.scatter(X[:, 0], X[:, 1], c=y_agg_clust, s=50, cmap='viridis')

# 绘制聚类中心（可选）
plt.title("Agglomerative Clustering")
plt.show()

# 绘制树状图
linked = linkage(X, 'ward')  # 计算层次聚类的链接矩阵
plt.figure(figsize=(10, 7))
dendrogram(linked)
plt.title("Dendrogram")
plt.show()

```

- **`make_blobs`**：生成一个简单的二维数据集，这里有 4 个聚类中心。
- **`AgglomerativeClustering`**：
    - `n_clusters=4`：指定最终的聚类数目为 4。
    - `affinity='euclidean'`：使用欧氏距离计算数据点间的距离。
    - `linkage='ward'`：使用 Ward 最小方差法（最常用的合并方式）来计算簇间的距离。
- **`fit_predict`**：训练模型并返回每个样本所属的聚类标签。
- **`dendrogram`**：绘制树状图，展示数据聚类过程。

## 1.5 小结

层次聚类是一种强大且灵活的聚类方法，尤其适用于不知道簇数且希望了解数据层次结构的场景。虽然其计算开销较大，但在一些数据集较小或聚类需求较为复杂的情况下，它仍然是一个非常有效的选择。通过合理调整簇间距离的度量方法和合并方式，层次聚类能够为我们提供直观且有意义的聚类结果。