---
sidebar_position: 6
---


# 支持向量机

支持向量机（SVM）是一种广泛应用于分类和回归任务的监督学习算法。SVM的核心思想是通过构造一个超平面来实现数据的分类，并尽量使得两类数据之间的间隔（margin）最大化，从而提高模型的泛化能力。

## 1.1 原理

支持向量机的目标是找到一个能够分隔数据的最佳超平面。假设数据集是线性可分的，SVM的目标是找到一个超平面，该平面能够最大化分类间隔，即**支持向量到超平面的距离**。

### 1.1.1 线性SVM分类器

- **超平面**

    对于二分类问题，假设数据集的特征向量为$x=(x_1,x_2,...,x_n)$，每个样本的标签为$y\in\{-1,1\}$，那么可以通过如下方式表示超平面：

$$
w\cdot x+b=0 
$$

    其中，$w$是超平面的法向量，$b$是偏置项。
- **支持向量**：支持向量是距离超平面最近的样本点，这些样本点对决定分隔超平面起着关键作用。
- **最大间隔**：我们要找到一个超平面，使得两类数据到超平面的间隔最大化。这个间隔可以通过以下约束来优化：
    - 对于标签为+1的样本点$x_i$，满足：

$$
w\cdot x_i+b\geq1
$$
    - 对于标签为-1的样本点$x_i$，满足：

$$
w\cdot x_i+b\leq1
$$
    - 在这两个约束下，目标是最大化间隔$\frac{2}{||w||}$，即最小化$||w||$：

$$
\min _{\mathbf{w}, b} \frac{1}{2}\|\mathbf{w}\|^{2}
$$
    - 同时，约束条件是：

$$
y_{i}\left(\mathbf{w} \cdot \mathbf{x}_{i}+b\right) \geq 1, \quad \forall i
$$
- **软间隔（Soft Margin）**

    在实际应用中，数据往往不可线性分割。因此，SVM引入了**软间隔**的概念，通过引入松弛变量$\xi_i$来允许某些样本点位于间隔边界内，从而实现对非线性可分数据的处理。软间隔的目标是优化间隔，并对分类错误进行惩罚。

    目标函数变为：

$$
\min _{\mathbf{w}, b, \xi} \frac{1}{2}\|\mathbf{w}\|^{2}+C \sum_{i=1}^{n} \xi_{i}
$$

    其中，$C$是一个惩罚参数，控制间隔大小与分类错误之间的权衡，$\xi_i$是第$i$个样本点的松弛变量，表示它距离正确分类超平面的偏差。

    约束条件为：

$$
y_{i}\left(\mathbf{w} \cdot \mathbf{x}_{i}+b\right) \geq 1-\xi_{i}, \quad \xi_{i} \geq 0
$$
- **核技巧**

    当数据不可线性分割时，SVM可以通过核技巧将数据映射到高维空间，使得在高维空间中可以找到一个超平面进行分割。常见的核函数包括：

    **线性核**：$K\left(\mathbf{x}, \mathbf{x}^{\prime}\right)=\mathbf{x}^{T} \mathbf{x}^{\prime}$

    **高斯径向基函数（RBF）核**：$K\left(\mathbf{x}, \mathbf{x}^{\prime}\right)=\exp \left(-\frac{\left\|\mathbf{x}-\mathbf{x}^{\prime}\right\|^{2}}{2 \sigma^{2}}\right)$

    **多项式核**：$K\left(\mathbf{x}, \mathbf{x}^{\prime}\right)=\left(\mathbf{x}^{T} \mathbf{x}^{\prime}+c\right)^{d}$

    核函数的引入使得SVM能够在高维空间中进行有效的分类，而无需显式地计算高维映射。
- **超参数**
    - **C**：控制软间隔的大小，值越大，惩罚项对错误分类的惩罚越强，可能导致过拟合；值越小，间隔更大，但可能导致欠拟合。
    - **核函数**：选择适合数据的核函数（线性核、RBF核、多项式核等）。
    - **gamma**：对于RBF核来说，决定了数据点对其影响的范围。值较小表示“远”的数据点也有较大影响，值较大表示只有近邻数据点才有影响。
    - **degree**：对于多项式核，控制多项式的次数。

## 1.2 应用

SVM具有很好的泛化能力，适用于许多分类和回归问题。典型的应用场景包括：

- **文本分类**：如垃圾邮件过滤、情感分析等。
- **图像识别**：人脸识别、手写数字识别等。
- **生物信息学**：基因数据分析、疾病预测等。
- **金融领域**：信用评分、风险管理等。
- **语音识别**：语音分类、语音到文本转换等。

## 1.3 优缺点

### 1.3.1 优点

- **高维空间有效**：SVM能有效处理高维数据，且在高维空间中依然能保持较好的计算效率。
- **鲁棒性强**：SVM通过最大化间隔的方式增强了模型的泛化能力，尤其适合小样本数据。
- **核技巧强大**：通过核技巧，SVM能够在非线性数据上表现良好。
- **不易过拟合**：SVM对于数据噪声的鲁棒性较强，尤其在数据量小且噪声较少时表现更佳。

### 1.3.2 缺点

- **计算开销大**：SVM的训练过程，尤其是使用核函数时，计算开销较大，训练时间较长。
- **对大数据集不适用**：当数据集非常庞大时，SVM的训练效率较低。
- **参数调优困难**：SVM的性能依赖于合适的核函数、惩罚参数CCC和核函数参数（如σ\sigmaσ），需要较多的实验来调整。
- **模型解释性差**：尽管SVM的理论基础很清晰，但它的决策过程不容易直观理解。

## 1.4 Scikit-Learn实现

```Python
import numpy as np
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

# 加载鸢尾花数据集
iris = datasets.load_iris()
X = iris.data
y = iris.target

# 划分数据集为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 创建SVM分类器，使用RBF核
clf = SVC(kernel='rbf', C=1, gamma=0.5)

# 训练模型
clf.fit(X_train, y_train)

# 预测
y_pred = clf.predict(X_test)

# 输出准确率
print(f"Accuracy: {accuracy_score(y_test, y_pred)}")

# 查看支持向量
print("Support vectors:", clf.support_vectors_)

```

- `SVC(kernel='rbf', C=1, gamma=0.5)`：创建一个使用RBF核的SVM分类器，`C=1`为惩罚参数，`gamma=0.5`为RBF核的参数。
- `fit`：训练SVM模型。
- `predict`：使用训练好的模型对测试集进行预测。
- `accuracy_score`：评估模型的分类准确率。
- `support_vectors_`：查看支持向量，帮助理解模型是如何做出决策的。

## 1.5 扩展与改进

### 1.5.1 多分类（One-vs-One，One-vs-Rest）

SVM 是一个二分类模型，原生并不支持多分类任务。然而，通过一些策略，我们可以将 SVM 扩展到多类分类问题。常用的两种方法是：

- **一对多（One-vs-Rest，OvR）**：将一个多类问题转化为多个二分类问题，每个类都与其他类进行比较。也就是说，对于$K$类问题，训练$K$个 SVM，每个 SVM 将一个类别作为正类，其他类别作为负类。最终，预测时，选择得分最高的分类器作为输出。
- **一对一（One-vs-One，OvO）**：将一个多类问题转化为每对类别的二分类问题。对于$K$个类别，会训练$(\frac{K\cdot(K-1)}{2})$个二分类 SVM。例如，对于三类分类问题，会有三个二分类模型：类1 vs 类2、类1 vs 类3、类2 vs 类3。最终预测时，采用投票机制决定类别。

在`scikit-learn`中，`SVC`默认使用一对一（OvO）策略来处理多类问题，但也可以通过设置`decision_function_shape='ovr'`来显式选择一对多（OvR）方法。

### 1.5.2 支持向量回归（SVR）

与 SVM 分类类似，**支持向量回归（SVR）**是 SVM 的一种扩展，旨在处理回归问题。SVR 的目标是找到一个能够使大多数数据点位于一个规定的容忍度（epsilon）范围内的回归平面，同时尽量减少偏差。

SVR的优化目标与 SVM 类似，只不过这里的目标是最小化回归误差，而不是分类误差。其优化目标通常为：

$$
\min _{\mathbf{w}, b, \epsilon} \frac{1}{2}\|\mathbf{w}\|^{2}+C \sum_{i=1}^{n}\left(\xi_{i}+\xi_{i}^{*}\right)
$$

其中，$\xi_i$和$x_{i}^{*}$是两个松弛变量，用于允许部分数据点偏离回归平面。通过惩罚这些偏差，控制回归模型的复杂度。

在`scikit-learn`中，`SVR`类似于`SVC`，可以使用 RBF 核进行回归：

```Python
from sklearn.svm import SVR
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# 创建回归数据
X, y = make_regression(n_samples=100, n_features=1, noise=0.1)

# 划分数据集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 创建并训练SVR模型
svr = SVR(kernel='rbf', C=100, gamma=0.1, epsilon=0.01)
svr.fit(X_train, y_train)

# 预测
y_pred = svr.predict(X_test)

# 计算均方误差
mse = mean_squared_error(y_test, y_pred)
print("Mean Squared Error:", mse)

```

### 1.5.3 核函数的选择和优化

SVM的一个关键部分是**核函数**的选择。核函数的作用是将数据从原始空间映射到更高维的特征空间，使得数据在高维空间中变得线性可分。常用的核函数有：

**线性核**：适用于数据线性可分的情况。核函数为$K\left(\mathbf{x}, \mathbf{x}^{\prime}\right)=\mathbf{x}^{T} \mathbf{x}^{\prime}$

**RBF核（径向基函数）**：适用于大多数数据。核函数为$K\left(\mathbf{x}, \mathbf{x}^{\prime}\right)=\exp \left(-\frac{\left\|\mathbf{x}-\mathbf{x}^{\prime}\right\|^{2}}{2 \sigma^{2}}\right)$，它通过控制$\sigma$参数来调整高维空间中的数据映射程度。

**多项式核**：适用于数据具有多项式关系的情况。核函数为$K\left(\mathbf{x}, \mathbf{x}^{\prime}\right)=\left(\mathbf{x}^{T} \mathbf{x}^{\prime}+c\right)^{d}$

**Sigmoid核**：适用于某些神经网络的模型，通常用于实验中。

选择合适的核函数对于SVM的性能至关重要。在实际应用中，可以通过交叉验证来选择最佳的核函数和参数。

### 1.5.4 参数调优

SVM模型的性能高度依赖于超参数的设置，最重要的超参数包括：

- **C**：惩罚参数，控制分类边界的宽度和对错误分类的容忍度。较大的 C 值表示对误分类的惩罚较大，可能导致过拟合；较小的 C 值则可能导致欠拟合。
- **gamma**：核函数的参数，控制数据点对决策边界的影响。较大的 gamma 值表示数据点对决策边界的影响范围较小，可能导致过拟合；较小的 gamma 值则可能导致欠拟合。
- **kernel**：核函数类型，可以选择线性核、RBF核、多项式核等。

这些超参数可以通过**网格搜索（Grid Search）**或**随机搜索（Random Search）**来优化。例如：

```Python
from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC

# 创建SVM模型
svc = SVC()

# 设置参数网格
param_grid = {'C': [0.1, 1, 10], 'gamma': [0.01, 0.1, 1], 'kernel': ['rbf', 'linear']}

# 使用GridSearchCV进行参数调优
grid_search = GridSearchCV(svc, param_grid, cv=5)
grid_search.fit(X_train, y_train)

# 输出最佳参数
print("Best parameters:", grid_search.best_params_)

```

### 1.5.5 SVM的扩展：半监督学习

在某些情况下，我们只有部分标注数据，SVM也可以扩展为半监督学习模型，进行**半监督支持向量机（Semi-Supervised SVM）**学习。通过利用无标签数据的信息，SVM可以有效地提高模型的准确性，尤其是当标注数据稀缺时。

## 1.6 小结

支持向量机（SVM）是一个强大的分类和回归算法，尤其适用于高维数据和小样本学习任务。它通过寻找一个最佳的超平面来最大化类间间隔，具有良好的泛化能力。虽然SVM的训练速度相对较慢，并且对大规模数据不太适用，但它仍然在许多实际问题中表现出色。

SVM具有以下特点：

- 在小样本、高维数据上表现优异。
- 可以使用不同的核函数解决非线性问题。
- 需要仔细调优参数（如 C 和 gamma）。
- 训练时间较长，尤其是在数据量较大的时候。

通过适当的扩展和优化，SVM不仅能处理标准的二分类问题，还能够处理回归、多类分类、甚至半监督学习问题，具有广泛的应用前景。