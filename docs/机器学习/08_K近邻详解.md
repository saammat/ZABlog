---
sidebar_position: 8
---


# K近邻

**K近邻（K-Nearest Neighbor，KNN）**是一种基于实例的机器学习算法，它的基本思想是：对于一个新的样本，KNN 会在训练数据集中找到距离该样本最近的$K$个邻居，然后根据这些邻居的类别来确定该样本的类别。KNN 是一种懒学习（Lazy Learning）算法，因为它不通过训练过程来建立模型，而是直接利用训练数据进行分类或回归。

## 1.1 原理

KNN 通过计算样本之间的距离来进行分类。对于一个待分类的样本，KNN 会选择训练集中与该样本距离最小的$K$个邻居，然后通过邻居的多数类别来决定该样本的类别。如果是回归任务，则取邻居的平均值作为预测值。

### 1.1.1 距离度量

常用的距离度量方式包括欧氏距离、曼哈顿距离和闵可夫斯基距离。最常见的方式是使用欧氏距离。

- **欧式距离**

    对于两个样本$x=(x_1,x_2,...,x_n)$和$x^{\prime}=\left(x_{1}^{\prime}, x_{2}^{\prime}, \ldots, x_{n}^{\prime}\right)$，欧式距离的计算公式为：

$$
d\left(x, x^{\prime}\right)=\sqrt{\sum_{i=1}^{n}\left(x_{i}-x_{i}^{\prime}\right)^{2}}
$$
- **曼哈顿距离**（L1距离）

$$
d\left(x, x^{\prime}\right)=\sum_{i=1}^{n}\left|x_{i}-x_{i}^{\prime}\right|
$$
- **闵可夫斯基距离**（是欧氏和曼哈顿距离的推广）

$$
d\left(x, x^{\prime}\right)=\left(\sum_{i=1}^{n}\left|x_{i}-x_{i}^{\prime}\right|^{p}\right)^{1 / p}
$$

### 1.1.2 计算过程

- 计算待分类样本与所有训练样本的距离。
- 找到距离最近的$K$个邻居。
- 根据这$K$个邻居的类别，通过**投票机制**决定待分类样本的类别。如果是回归问题，则通过取这$K$个邻居的平均值来预测结果。
- **分类计算公式**

    如果有$K$个邻居，分别属于类别$C_1,C_2,...,C_K$，则待分类样本的类别$C $为：

$$
C=\arg \max _{C_{k}} \sum_{i=1}^{K} \mathbb{I}\left(C_{k}=C_{i}\right)
$$

    其中，$\mathbb{I}\left(C_{k}=C_{i}\right)$是指示函数，表示如果$C_k$等于$C_i$，则为1，否则为0。
- **回归计算公式**

    对于回归任务，KNN 会计算$K$个邻居的平均值作为预测结果。设 $y_1,y_2,...,y_K$是最近邻的标签值（即预测值），则预测值为：

$$
y_{\mathrm{pred}}=\frac{1}{K} \sum_{i=1}^{K} y_{i}
$$

## 1.2 应用

KNN 广泛应用于许多机器学习任务中，特别是在以下场景下效果显著：

- **分类任务**
    - **图像识别**：在图像分类问题中，KNN 可以用于根据像素值的相似度对图片进行分类。
    - **文本分类**：根据文本中单词的分布情况来进行分类，常用于垃圾邮件过滤、情感分析等。
    - **手写数字识别**：KNN 可以用于将手写数字图像与预先标注的数字进行匹配，进行数字识别。
- **回归任务**
    - **房价预测**：根据邻近区域的房价来预测某一地区的房价。
    - **天气预报**：根据历史天气数据来预测未来的天气情况。
- **推荐系统**：基于用户的兴趣和行为相似性，KNN 可以进行个性化推荐。

## 1.3 优缺点

### 1.3.1 优点

- **简单易理解**：KNN 算法非常直观，易于理解和实现。
- **无训练过程**：KNN 是一个懒学习算法，模型训练过程非常简单，只需要存储训练数据即可。适合实时应用。
- **适用于多分类问题**：KNN 可以自然地扩展到多类别问题，只需选择合适的$K$值即可。
- **适用于非线性决策边界**：KNN 不依赖于数据的假设，能够处理复杂的非线性问题。

### 1.3.2 缺点

- **计算开销大**：由于 KNN 需要计算测试样本与所有训练样本之间的距离，计算量大，尤其在数据量大的时候，训练和预测时间都较长。
- **存储需求大**：KNN 需要存储所有训练数据，内存开销较大。
- **对噪声敏感**：KNN 对数据中的噪声（异常点）较为敏感，噪声可能影响距离的计算，导致分类效果下降。
- **高维数据表现差**：在高维空间中，数据的稀疏性增加，KNN 的表现可能会急剧下降，称为“维度灾难”。
- **需要选择合适的KKK值**：选择不当的$K$值可能导致欠拟合或过拟合。

### 1.3.3 优化与改进

- **特征缩放**：由于 KNN 计算距离时依赖于特征之间的差异，因此对于不同尺度的特征，KNN 可能会受到影响。常见的做法是对数据进行标准化或归一化处理，使得每个特征具有相同的尺度。
- **加权 KNN**：传统的 KNN 算法是对 K 个邻居等权投票或平均值，但有时可以对更近的邻居赋予更大的权重，这样可以使得距离较近的样本对结果的影响更大。
- **使用KD树或Ball树加速计算**：为了加速 KNN 的计算，尤其是在高维空间中，可以使用 KD树或Ball树来进行高效的最近邻搜索。

## 1.4 Scikit-Learn实现

### 1.4.1 分类demo

```Python
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# 加载鸢尾花数据集
iris = load_iris()
X = iris.data
y = iris.target

# 划分数据集为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 创建 KNN 分类器
knn = KNeighborsClassifier(n_neighbors=3)

# 训练模型
knn.fit(X_train, y_train)

# 预测
y_pred = knn.predict(X_test)

# 输出准确率
print("KNN Classification Accuracy:", accuracy_score(y_test, y_pred))

```

### 1.4.2 回归demo

```Python
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error

# 创建回归数据
X, y = make_regression(n_samples=100, n_features=1, noise=0.1)

# 划分数据集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 创建 KNN 回归器
knn_reg = KNeighborsRegressor(n_neighbors=3)

# 训练模型
knn_reg.fit(X_train, y_train)

# 预测
y_pred_reg = knn_reg.predict(X_test)

# 计算均方误差
mse = mean_squared_error(y_test, y_pred_reg)
print("KNN Regression Mean Squared Error:", mse)

```

## 1.5 小结

K近邻（KNN）是一种简单且有效的分类和回归算法，适用于小规模数据集以及高维数据的分类问题。它的主要优点是易于理解和实现，并且不需要显式的训练过程。然而，KNN 在计算上非常依赖数据的规模和维度，因此在大规模数据集上可能面临性能瓶颈。在实际应用中，KNN 在文本分类、图像识别和推荐系统等领域取得了很好的效果。