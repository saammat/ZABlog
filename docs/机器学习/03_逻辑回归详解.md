---
sidebar_position: 3
---

# 逻辑回归

**逻辑回归（Logistic Regression）是一种用于分类问题的统计学方法，尽管其名称中包含“回归”二字，但它实际上是一种分类模型。逻辑回归的目标是预测某一事件的概率，通常用于二分类、多分类问题（例如，预测某个邮件是否是垃圾邮件，患者是否患病等）。**

与线性回归不同，逻辑回归通过使用**逻辑函数（Logistic Function）**将线性回归的输出映射到一个介于0和1之间的概率值，从而将输出转换为分类标签（如0或1）。

## 1.1 原理

逻辑回归的核心思想是通过一个线性回归模型预测一个数值，然后将其输入到逻辑函数中，将其转化为一个概率值。模型通过学习数据中的特征来估计事件发生的概率。

在**二分类**问题中，逻辑回归输出的是一个0~1之间的概率值，表示样本属于正例的概率，因此可以根据设定的阈值将样本分为两类。

在**多分类**问题中，逻辑回归可以使用softmax函数将多个概率值归一化，并输出每个类别的概率分布。

### 1.1.1 二分类典型公式

逻辑回归的目标是预测事件发生的概率$P(y=1|X)$。其公式为：

$$
P(y=1|X)=\sigma(\omega_{0}+\omega_{1}x_{1}+\omega_{2}x_{2}+\cdot\cdot\cdot+\omega_{p}x_{p})
$$

其中：

- $P(y=1|X)$是目标变量$y$取值为1的条件概率（即预测为正类的概率）。
- $\sigma(x)$是**Sigmoid函数**（也叫Logistic函数），其公式为：

$$
\sigma(z)=\frac{1}{1+e^{-z}}
$$
- $\omega_1, \omega_2, ... , \omega_p$是回归模型的参数（系数）。
- $x_1, x_2, ... , x_p$是多个输入特征。

逻辑回归的输出$P(y=1|X)$是一个概率值，表示事件发生的概率。如果该概率大于某个阈值（通常为0.5），则预测为正类（1），否则预测为负类（0）。

逻辑回归使用的是sigmoid函数作为激活函数，将输入特征线性加权求和后，再经过sigmoid函数转换成0~1之间的概率值。

### 1.1.2 二分类交叉熵损失函数

逻辑回归使用的损失函数是交叉熵（Cross Entropy），在训练过程中，逻辑回归的目标是最小化**交叉熵损失**，也叫**对数损失**（Log Loss）。它的目标是最小化预测值与真实标签之间的差距。交叉熵损失函数的公式为：

$$
Loss=-\frac{1}{n}\sum_{i=1}^{n}{[y_{i}log(\hat{y_{i}})+(1-y_{i})log(1-\hat{y_{i}}))]}
$$

其中：

- $y_i
 $是真实值。
- $\hat{y_i}$是模型预测值。

通过最小化这个损失函数，逻辑回归能够优化模型参数，使得模型在训练数据上表现更好。

### 1.1.3 多分类典型公式

逻辑回归的目标是预测事件发生的概率$P(y=k|X)$。其公式为：

$$
P(y=k|X)=\sigma(\omega_{0}+\omega_{1}x_{1}+\omega_{2}x_{2}+\cdot\cdot\cdot+\omega_{p}x_{p})
$$

其中：

- $P(y=k|X)$是目标变量$y$取值为$k$的条件概率（即预测为正类的概率）。
- $\sigma(x)$是**Softmax函数**。通常，逻辑回归使用**Softmax 函数**作为激活函数，来确保预测的概率和为 1。**Softmax**是一个扩展的 Sigmoid 函数，用于多类分类。它将每个类别的原始得分（logits）转换为一个概率分布。对于每个类别 𝑘，其概率由以下公式计算：

$$
P(y=k|X)=\frac{e^{z_{k}}}{\sum_{j=1}^{K}{e^{z_{j}}}}
$$

    其中：

    $z_{k}=\beta_{k}^{T}X $是类别$k$的得分（与输入特征的加权和）

    K是类别总数

    X是输入特征向量
- 模型输出是每个类别的概率，通常会选择最大概率的类别作为最终预测结果。

### 1.1.4 多分类交叉熵损失函数

多分类逻辑回归使用**交叉熵损失函数**，即：

$$
Loss=-\sum_{i=1}^{n}{\sum_{k=1}^{K}{y_{i,k}log(\bar{y_{i,k}})}}
$$

其中：

- $y_i,k$是第i个样本对于类别$k$的真实标签（在 one-hot 编码中为 1 或 0）。
- $\hat{y_i},k$是第i个样本对于类别$k$的预测概率。

## 1.2 应用

逻辑回归广泛应用于二分类、多分类问题。以下是一些典型的应用场景：

- **疾病预测**：根据患者的体征数据预测是否患病（例如预测某个癌症是否发生）。
- **金融风控**：根据借款人的个人信息预测是否会违约（例如信用评分预测）。
- **垃圾邮件分类**：预测一封邮件是否为垃圾邮件（Spam vs. Not Spam）。
- **广告点击率预测**：预测广告是否会被用户点击（点击/不点击）。
- **客户流失预测**：预测某个用户是否会取消订阅或停止使用产品。

## 1.3 优缺点

### 1.3.1 优点

- 简单易懂：逻辑回归是一个非常简单的模型，适用于线性可分的二分类问题，且易于实现和理解。
- 计算效率高：训练速度较快，适合数据量较大的任务。
- 概率输出：与决策树等模型不同，逻辑回归能够输出概率，便于进一步的分析和决策。
- 可解释性强：逻辑回归的系数能够清晰地表示各个特征对结果的影响程度，易于解释和分析。

### 1.3.2 缺点

- 线性假设：逻辑回归假设特征和目标之间存在线性关系，当数据不满足这个假设时，模型表现可能较差。
- 不适合复杂非线性问题：对于非线性问题，逻辑回归可能无法捕捉复杂的关系。
- 对特征缩放敏感：如果特征的尺度差异很大，需要进行标准化或归一化，否则可能会影响模型的表现。
- 对异常值敏感：逻辑回归对异常值比较敏感，异常值可能会影响系数的估计。

## 1.4 Scikit-Learn实现

安装scikit-learn：

```Bash
pip install scikit-learn

```

单分类实现demo：

```Python
# 导入必要的库
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix

# 加载示例数据集（鸢尾花数据集）
data = load_iris()
X = data.data[:, :2]  # 选择前两个特征
y = (data.target != 0) * 1  # 只选择二分类任务（是否为Setosa）

# 数据拆分：70%训练集，30%测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 创建逻辑回归模型
model = LogisticRegression()

# 训练模型
model.fit(X_train, y_train)

# 预测
y_pred = model.predict(X_test)

# 模型评估
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)

print(f"准确率: {accuracy:.2f}")
print("混淆矩阵:")
print(conf_matrix)

# 可视化决策边界
h = .02  # 网格步长
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))

Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

plt.contourf(xx, yy, Z, alpha=0.8)
plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', marker='o', s=50, cmap=plt.cm.RdYlBu)
plt.xlabel('特征1')
plt.ylabel('特征2')
plt.title('逻辑回归 - 决策边界')
plt.show()

```

- **数据加载**：使用 Scikit-Learn 提供的鸢尾花数据集，并通过数据预处理将其转换为二分类任务（是否为Setosa）。
- **数据拆分**：使用`train_test_split`将数据拆分为训练集和测试集。
- **模型训练**：创建`LogisticRegression`模型并通过`.fit()`方法训练模型。
- **预测与评估**：使用`.predict()`方法进行预测，并通过`accuracy_score`和`confusion_matrix`评估模型的表现。
- **可视化**：通过绘制决策边界图来可视化模型的分类效果。

多分类实现demo：

```Python
# 导入必要的库
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix

# 加载鸢尾花数据集
data = load_iris()
X = data.data
y = data.target  # 目标变量有三类

# 数据拆分：70%训练集，30%测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 创建逻辑回归模型（默认使用一对多策略进行多分类）
model = LogisticRegression(max_iter=200)

# 训练模型
model.fit(X_train, y_train)

# 预测
y_pred = model.predict(X_test)

# 模型评估
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)

print(f"准确率: {accuracy:.2f}")
print("混淆矩阵:")
print(conf_matrix)

# 可视化混淆矩阵
import seaborn as sns
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=data.target_names, yticklabels=data.target_names)
plt.xlabel('预测标签')
plt.ylabel('真实标签')
plt.title('混淆矩阵')
plt.show()

```

- **数据加载**：我们使用 Scikit-learn 自带的**鸢尾花数据集**（Iris dataset），这个数据集有三个类别的花卉，并且每个花卉样本有 4 个特征。
- **数据拆分**：通过`train_test_split`将数据拆分为训练集和测试集。
- **模型训练**：创建一个`LogisticRegression`模型，默认使用**一对多**策略进行多分类。
- **模型评估**：通过`accuracy_score`计算准确率，并通过`confusion_matrix`计算混淆矩阵，以评估模型在每个类别上的表现。
- **可视化**：使用`seaborn`库绘制混淆矩阵，帮助直观地观察模型的分类效果。

## 1.5 小结

逻辑回归是一个广泛应用于二分类问题的经典模型，它简单、易于理解，并且能够输出概率。然而，逻辑回归假设特征与目标之间存在线性关系，因此它可能无法很好地处理复杂的非线性问题。它也对特征缩放和异常值较为敏感。在实际应用中，逻辑回归常常作为基准模型，与其他更复杂的模型进行对比。